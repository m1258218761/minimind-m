{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92f4b5be-6549-4229-b143-82feb521b028",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T13:43:02.256614Z",
     "start_time": "2025-01-21T13:42:54.214865Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600000it [00:07, 75028.33it/s] \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "train_data = []\n",
    "with open(\"tokenizer_train.jsonl\", \"r\") as fr:\n",
    "    for line in tqdm(fr):\n",
    "        line = json.loads(line)\n",
    "        train_data.append(line)\n",
    "def yield_text():\n",
    "    for line in train_data:\n",
    "        yield line[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f14f7602-0b66-48c7-898a-bc106a3cc97c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T13:43:15.204003Z",
     "start_time": "2025-01-21T13:43:15.195662Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '好的。现在请你将这个文本中的所有的逗号都替换成空格。 好的，请稍等一下，现在我会将文本中的所有逗号替换为空格。处理后文本为：\"这是一个句子 目的是看看是否可以正确地从这个句子中删除关键词。\"。处理结果如何？'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13160ff-9f8e-416b-a7b6-0b61b3b7704e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-21T13:50:47.477871Z"
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tokenizers import Tokenizer, pre_tokenizers, decoders\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "\n",
    "special_tokens = [\"<unk>\", \"<s>\", \"</s>\"]\n",
    "trainer = BpeTrainer(\n",
    "    vocab_size=8000,\n",
    "    special_tokens=special_tokens,  # 确保这三个token被包含\n",
    "    show_progress=True,\n",
    "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
    ")\n",
    "\n",
    "texts = yield_text()\n",
    "tokenizer.train_from_iterator(texts, trainer=trainer)\n",
    "tokenizer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca25149b-9dd9-4852-ba9e-ba810c305d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置解码器\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# 检查特殊token的索引\n",
    "assert tokenizer.token_to_id(\"<unk>\") == 0\n",
    "assert tokenizer.token_to_id(\"<s>\") == 1\n",
    "assert tokenizer.token_to_id(\"</s>\") == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bfeca97-1d06-4dc3-b985-650fe7ed9f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./my_tokenizer/vocab.json', './my_tokenizer/merges.txt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dir = \"./my_tokenizer\"\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "tokenizer.save(os.path.join(tokenizer_dir, \"tokenizer.json\"))\n",
    "tokenizer.model.save(\"./my_tokenizer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f512ee5-abc2-4a37-a270-e0b8238e1388",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 手动创建配置文件\n",
    "    config = {\n",
    "        \"add_bos_token\": False,\n",
    "        \"add_eos_token\": False,\n",
    "        \"add_prefix_space\": True,\n",
    "        \"added_tokens_decoder\": {\n",
    "            \"0\": {\n",
    "                \"content\": \"<unk>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\n",
    "            \"1\": {\n",
    "                \"content\": \"<s>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            },\n",
    "            \"2\": {\n",
    "                \"content\": \"</s>\",\n",
    "                \"lstrip\": False,\n",
    "                \"normalized\": False,\n",
    "                \"rstrip\": False,\n",
    "                \"single_word\": False,\n",
    "                \"special\": True\n",
    "            }\n",
    "        },\n",
    "        \"additional_special_tokens\": [],\n",
    "        \"bos_token\": \"<s>\",\n",
    "        \"clean_up_tokenization_spaces\": False,\n",
    "        \"eos_token\": \"</s>\",\n",
    "        \"legacy\": True,\n",
    "        \"model_max_length\": 1000000000000000019884624838656,\n",
    "        \"pad_token\": None,\n",
    "        \"sp_model_kwargs\": {},\n",
    "        \"spaces_between_special_tokens\": False,\n",
    "        \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
    "        \"unk_token\": \"<unk>\",\n",
    "        \"use_default_system_prompt\": False,\n",
    "        \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<s>user\\\\n' + content + '</s>\\\\n<s>assistant\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '</s>' + '\\\\n' }}{% endif %}{% endfor %}\"\n",
    "    }\n",
    "\n",
    "    # 保存配置文件\n",
    "    with open(os.path.join(tokenizer_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as config_file:\n",
    "        json.dump(config, config_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a34a06610534c32",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T13:44:49.329773Z",
     "start_time": "2025-01-21T13:44:49.287597Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='./my_tokenizer', vocab_size=8000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n",
      "你是一个优秀的聊天机器人，总是给我正确的回应！<s>user\n",
      "你来自哪里？</s>\n",
      "<s>assistant\n",
      "我来自地球</s>\n",
      "\n",
      "tokenizer实际词表长度： 8000\n",
      "encoder长度： 33\n",
      " 你是一个优秀的聊天机器人，总是给我正确的回应！<s> user\n",
      "你来自哪里？</s> \n",
      "<s> assistant\n",
      "我来自地球</s> \n",
      "\n",
      "decoder和原始文本是否一致： False\n"
     ]
    }
   ],
   "source": [
    "## 验证tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./my_tokenizer\")\n",
    "print(tokenizer)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个优秀的聊天机器人，总是给我正确的回应！\"},\n",
    "    {\"role\": \"user\", \"content\": '你来自哪里？'},\n",
    "    {\"role\": \"assistant\", \"content\": '我来自地球'}\n",
    "]\n",
    "new_prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False\n",
    ")\n",
    "print(new_prompt)\n",
    "\n",
    "# 获取实际词汇表长度（包括特殊符号）\n",
    "actual_vocab_size = len(tokenizer)\n",
    "print('tokenizer实际词表长度：', actual_vocab_size)\n",
    "\n",
    "model_inputs = tokenizer(new_prompt)\n",
    "print('encoder长度：', len(model_inputs['input_ids']))\n",
    "\n",
    "input_ids = model_inputs['input_ids']\n",
    "response = tokenizer.decode(input_ids)\n",
    "print(response)\n",
    "print('decoder和原始文本是否一致：', response == new_prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

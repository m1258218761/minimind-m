{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d47ce5e-c4bb-4fa3-a8bb-b039d44d2952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2Config {\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 48,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nLlamaConfig {\\n  'attention_bias': false,                 # 不使用注意力偏置\\n  'attention_dropout': 0.0,                # 注意力层的 dropout 比例\\n  'bos_token_id': 1,                       # bos_token (begin of sentence) 的 id\\n  'eos_token_id': 2,                       # eos_token (end of sentence) 的 id\\n  'hidden_act': 'silu',                    # 隐藏层激活函数类型，silu 即 SwiGLU\\n  'hidden_size': 256,                      # 隐藏层维度大小\\n  'initializer_range': 0.02,               # 权重初始化范围，会被后面的 Kaiming 初始化覆盖\\n  'intermediate_size': 768,                # 中间层大小，采用 8/3 倍而非 4 倍\\n  'max_position_embeddings': 2048,\\n  'model_type': 'llama',\\n  'num_attention_heads': 16,\\n  'num_hidden_layers': 4,\\n  'num_key_value_heads': 8,\\n  'pretraining_tp': 1,\\n  'rms_norm_eps': 1e-06,\\n  'rope_scaling': null,\\n  'rope_theta': 10000.0,\\n  'tie_word_embeddings': false,            # 头尾 embedding 和 lm_head 是否共享权重\\n  'transformers_version': '4.40.0',\\n  'use_cache': true,\\n  'vocab_size': 32000\\n}\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"7\"\n",
    "# 模型配置\n",
    "from transformers import AutoConfig\n",
    "\n",
    "hidden_size = 1024\n",
    "# 中间层取 8/3 倍，按 128 向上取整\n",
    "# intermediate_size = (int(hidden_size * 8/3 / 128) + 1) * 128\n",
    "intermediate_size = hidden_size * 4\n",
    "\n",
    "# 只改动我们需要调整的参数，其余保持不变\n",
    "config = AutoConfig.for_model(\n",
    "    model_type='qwen2',\n",
    "    hidden_size=hidden_size,\n",
    "    intermediate_size=intermediate_size,\n",
    "    num_attention_heads=16,\n",
    "    num_hidden_layers=48,\n",
    "    num_key_value_heads=8                  # 分为 8 组\n",
    ")\n",
    "print(config)\n",
    "\n",
    "'''\n",
    "LlamaConfig {\n",
    "  'attention_bias': false,                 # 不使用注意力偏置\n",
    "  'attention_dropout': 0.0,                # 注意力层的 dropout 比例\n",
    "  'bos_token_id': 1,                       # bos_token (begin of sentence) 的 id\n",
    "  'eos_token_id': 2,                       # eos_token (end of sentence) 的 id\n",
    "  'hidden_act': 'silu',                    # 隐藏层激活函数类型，silu 即 SwiGLU\n",
    "  'hidden_size': 256,                      # 隐藏层维度大小\n",
    "  'initializer_range': 0.02,               # 权重初始化范围，会被后面的 Kaiming 初始化覆盖\n",
    "  'intermediate_size': 768,                # 中间层大小，采用 8/3 倍而非 4 倍\n",
    "  'max_position_embeddings': 2048,\n",
    "  'model_type': 'llama',\n",
    "  'num_attention_heads': 16,\n",
    "  'num_hidden_layers': 4,\n",
    "  'num_key_value_heads': 8,\n",
    "  'pretraining_tp': 1,\n",
    "  'rms_norm_eps': 1e-06,\n",
    "  'rope_scaling': null,\n",
    "  'rope_theta': 10000.0,\n",
    "  'tie_word_embeddings': false,            # 头尾 embedding 和 lm_head 是否共享权重\n",
    "  'transformers_version': '4.40.0',\n",
    "  'use_cache': true,\n",
    "  'vocab_size': 32000\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd68522-c70e-4f5b-b4e7-c94009f40748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# 能用 cuda 就用 cuda\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# 从配置加载模型\n",
    "model = AutoModelForCausalLM.from_config(\n",
    "    config,\n",
    "    torch_dtype=torch.float32   # 全精度训练\n",
    ").to(device)                    # 迁移到 device 上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "307fc4b0-fdff-42e9-be53-a1efec59bb21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name & Parameters\n",
      "----------------------------\n",
      "model.embed_tokens.weight                          | Size: torch.Size([151936, 1024])     | Count: 155582464           \n",
      "model.layers.0.self_attn.q_proj.weight             | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.0.self_attn.q_proj.bias               | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.0.self_attn.k_proj.weight             | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.0.self_attn.k_proj.bias               | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.0.self_attn.v_proj.weight             | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.0.self_attn.v_proj.bias               | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.0.self_attn.o_proj.weight             | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.0.mlp.gate_proj.weight                | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.0.mlp.up_proj.weight                  | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.0.mlp.down_proj.weight                | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.0.input_layernorm.weight              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.0.post_attention_layernorm.weight     | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.1.self_attn.q_proj.weight             | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.1.self_attn.q_proj.bias               | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.1.self_attn.k_proj.weight             | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.1.self_attn.k_proj.bias               | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.1.self_attn.v_proj.weight             | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.1.self_attn.v_proj.bias               | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.1.self_attn.o_proj.weight             | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.1.mlp.gate_proj.weight                | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.1.mlp.up_proj.weight                  | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.1.mlp.down_proj.weight                | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.1.input_layernorm.weight              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.1.post_attention_layernorm.weight     | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.2.self_attn.q_proj.weight             | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.2.self_attn.q_proj.bias               | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.2.self_attn.k_proj.weight             | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.2.self_attn.k_proj.bias               | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.2.self_attn.v_proj.weight             | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.2.self_attn.v_proj.bias               | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.2.self_attn.o_proj.weight             | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.2.mlp.gate_proj.weight                | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.2.mlp.up_proj.weight                  | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.2.mlp.down_proj.weight                | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.2.input_layernorm.weight              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.2.post_attention_layernorm.weight     | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.3.self_attn.q_proj.weight             | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.3.self_attn.q_proj.bias               | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.3.self_attn.k_proj.weight             | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.3.self_attn.k_proj.bias               | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.3.self_attn.v_proj.weight             | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.3.self_attn.v_proj.bias               | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.3.self_attn.o_proj.weight             | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.3.mlp.gate_proj.weight                | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.3.mlp.up_proj.weight                  | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.3.mlp.down_proj.weight                | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.3.input_layernorm.weight              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.3.post_attention_layernorm.weight     | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.4.self_attn.q_proj.weight             | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.4.self_attn.q_proj.bias               | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.4.self_attn.k_proj.weight             | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.4.self_attn.k_proj.bias               | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.4.self_attn.v_proj.weight             | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.4.self_attn.v_proj.bias               | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.4.self_attn.o_proj.weight             | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.4.mlp.gate_proj.weight                | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.4.mlp.up_proj.weight                  | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.4.mlp.down_proj.weight                | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.4.input_layernorm.weight              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.4.post_attention_layernorm.weight     | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.5.self_attn.q_proj.weight             | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.5.self_attn.q_proj.bias               | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.5.self_attn.k_proj.weight             | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.5.self_attn.k_proj.bias               | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.5.self_attn.v_proj.weight             | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.5.self_attn.v_proj.bias               | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.5.self_attn.o_proj.weight             | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.5.mlp.gate_proj.weight                | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.5.mlp.up_proj.weight                  | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.5.mlp.down_proj.weight                | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.5.input_layernorm.weight              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.5.post_attention_layernorm.weight     | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.6.self_attn.q_proj.weight             | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.6.self_attn.q_proj.bias               | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.6.self_attn.k_proj.weight             | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.6.self_attn.k_proj.bias               | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.6.self_attn.v_proj.weight             | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.6.self_attn.v_proj.bias               | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.6.self_attn.o_proj.weight             | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.6.mlp.gate_proj.weight                | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.6.mlp.up_proj.weight                  | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.6.mlp.down_proj.weight                | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.6.input_layernorm.weight              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.6.post_attention_layernorm.weight     | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.7.self_attn.q_proj.weight             | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.7.self_attn.q_proj.bias               | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.7.self_attn.k_proj.weight             | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.7.self_attn.k_proj.bias               | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.7.self_attn.v_proj.weight             | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.7.self_attn.v_proj.bias               | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.7.self_attn.o_proj.weight             | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.7.mlp.gate_proj.weight                | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.7.mlp.up_proj.weight                  | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.7.mlp.down_proj.weight                | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.7.input_layernorm.weight              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.7.post_attention_layernorm.weight     | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.8.self_attn.q_proj.weight             | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.8.self_attn.q_proj.bias               | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.8.self_attn.k_proj.weight             | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.8.self_attn.k_proj.bias               | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.8.self_attn.v_proj.weight             | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.8.self_attn.v_proj.bias               | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.8.self_attn.o_proj.weight             | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.8.mlp.gate_proj.weight                | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.8.mlp.up_proj.weight                  | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.8.mlp.down_proj.weight                | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.8.input_layernorm.weight              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.8.post_attention_layernorm.weight     | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.9.self_attn.q_proj.weight             | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.9.self_attn.q_proj.bias               | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.9.self_attn.k_proj.weight             | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.9.self_attn.k_proj.bias               | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.9.self_attn.v_proj.weight             | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.9.self_attn.v_proj.bias               | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.9.self_attn.o_proj.weight             | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.9.mlp.gate_proj.weight                | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.9.mlp.up_proj.weight                  | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.9.mlp.down_proj.weight                | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.9.input_layernorm.weight              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.9.post_attention_layernorm.weight     | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.10.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.10.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.10.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.10.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.10.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.10.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.10.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.10.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.10.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.10.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.10.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.10.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.11.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.11.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.11.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.11.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.11.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.11.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.11.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.11.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.11.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.11.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.11.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.11.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.12.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.12.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.12.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.12.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.12.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.12.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.12.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.12.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.12.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.12.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.12.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.12.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.13.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.13.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.13.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.13.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.13.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.13.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.13.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.13.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.13.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.13.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.13.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.13.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.14.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.14.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.14.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.14.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.14.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.14.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.14.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.14.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.14.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.14.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.14.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.14.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.15.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.15.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.15.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.15.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.15.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.15.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.15.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.15.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.15.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.15.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.15.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.15.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.16.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.16.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.16.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.16.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.16.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.16.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.16.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.16.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.16.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.16.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.16.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.16.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.17.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.17.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.17.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.17.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.17.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.17.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.17.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.17.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.17.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.17.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.17.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.17.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.18.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.18.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.18.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.18.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.18.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.18.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.18.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.18.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.18.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.18.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.18.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.18.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.19.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.19.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.19.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.19.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.19.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.19.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.19.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.19.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.19.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.19.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.19.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.19.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.20.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.20.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.20.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.20.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.20.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.20.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.20.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.20.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.20.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.20.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.20.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.20.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.21.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.21.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.21.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.21.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.21.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.21.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.21.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.21.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.21.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.21.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.21.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.21.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.22.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.22.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.22.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.22.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.22.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.22.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.22.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.22.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.22.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.22.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.22.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.22.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.23.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.23.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.23.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.23.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.23.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.23.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.23.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.23.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.23.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.23.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.23.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.23.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.24.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.24.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.24.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.24.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.24.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.24.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.24.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.24.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.24.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.24.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.24.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.24.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.25.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.25.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.25.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.25.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.25.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.25.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.25.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.25.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.25.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.25.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.25.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.25.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.26.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.26.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.26.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.26.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.26.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.26.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.26.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.26.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.26.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.26.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.26.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.26.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.27.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.27.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.27.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.27.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.27.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.27.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.27.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.27.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.27.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.27.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.27.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.27.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.28.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.28.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.28.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.28.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.28.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.28.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.28.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.28.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.28.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.28.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.28.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.28.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.29.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.29.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.29.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.29.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.29.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.29.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.29.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.29.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.29.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.29.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.29.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.29.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.30.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.30.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.30.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.30.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.30.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.30.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.30.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.30.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.30.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.30.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.30.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.30.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.31.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.31.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.31.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.31.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.31.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.31.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.31.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.31.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.31.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.31.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.31.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.31.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.32.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.32.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.32.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.32.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.32.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.32.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.32.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.32.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.32.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.32.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.32.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.32.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.33.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.33.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.33.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.33.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.33.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.33.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.33.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.33.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.33.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.33.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.33.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.33.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.34.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.34.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.34.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.34.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.34.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.34.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.34.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.34.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.34.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.34.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.34.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.34.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.35.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.35.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.35.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.35.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.35.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.35.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.35.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.35.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.35.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.35.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.35.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.35.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.36.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.36.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.36.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.36.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.36.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.36.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.36.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.36.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.36.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.36.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.36.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.36.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.37.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.37.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.37.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.37.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.37.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.37.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.37.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.37.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.37.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.37.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.37.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.37.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.38.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.38.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.38.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.38.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.38.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.38.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.38.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.38.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.38.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.38.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.38.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.38.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.39.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.39.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.39.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.39.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.39.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.39.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.39.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.39.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.39.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.39.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.39.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.39.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.40.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.40.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.40.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.40.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.40.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.40.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.40.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.40.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.40.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.40.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.40.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.40.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.41.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.41.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.41.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.41.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.41.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.41.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.41.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.41.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.41.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.41.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.41.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.41.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.42.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.42.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.42.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.42.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.42.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.42.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.42.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.42.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.42.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.42.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.42.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.42.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.43.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.43.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.43.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.43.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.43.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.43.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.43.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.43.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.43.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.43.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.43.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.43.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.44.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.44.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.44.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.44.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.44.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.44.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.44.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.44.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.44.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.44.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.44.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.44.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.45.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.45.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.45.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.45.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.45.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.45.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.45.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.45.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.45.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.45.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.45.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.45.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.46.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.46.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.46.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.46.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.46.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.46.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.46.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.46.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.46.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.46.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.46.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.46.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.47.self_attn.q_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.47.self_attn.q_proj.bias              | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.47.self_attn.k_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.47.self_attn.k_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.47.self_attn.v_proj.weight            | Size: torch.Size([512, 1024])        | Count: 524288              \n",
      "model.layers.47.self_attn.v_proj.bias              | Size: torch.Size([512])              | Count: 512                 \n",
      "model.layers.47.self_attn.o_proj.weight            | Size: torch.Size([1024, 1024])       | Count: 1048576             \n",
      "model.layers.47.mlp.gate_proj.weight               | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.47.mlp.up_proj.weight                 | Size: torch.Size([4096, 1024])       | Count: 4194304             \n",
      "model.layers.47.mlp.down_proj.weight               | Size: torch.Size([1024, 4096])       | Count: 4194304             \n",
      "model.layers.47.input_layernorm.weight             | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.layers.47.post_attention_layernorm.weight    | Size: torch.Size([1024])             | Count: 1024                \n",
      "model.norm.weight                                  | Size: torch.Size([1024])             | Count: 1024                \n",
      "lm_head.weight                                     | Size: torch.Size([151936, 1024])     | Count: 155582464           \n",
      "----------------------------\n",
      "Total Parameters: 1066337280 (1066.3 M)\n"
     ]
    }
   ],
   "source": [
    "# 打印模型的每一层及其参数大小\n",
    "def print_model_parameters(model):\n",
    "    print('Layer Name & Parameters')\n",
    "    print('----------------------------')\n",
    "    total_params = 0\n",
    "    for name, parameter in model.named_parameters():\n",
    "        param_size = parameter.size()\n",
    "        param_count = torch.prod(torch.tensor(param_size)).item()\n",
    "        total_params += param_count\n",
    "        print(f'{name:50} | Size: {str(param_size):30} | Count: {str(param_count):20}')\n",
    "    print('----------------------------')\n",
    "    print(f'Total Parameters: {total_params} ({total_params / 1000000:.1f} M)')\n",
    "\n",
    "print_model_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f7073dc-7b93-42a7-b073-9a500d65e383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaiming 初始化\n",
    "def kaiming_initialization(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name and param.dim() > 1:\n",
    "            torch.nn.init.kaiming_uniform_(param, mode='fan_in', nonlinearity='leaky_relu')\n",
    "        elif 'bias' in name:\n",
    "            # 一般偏置项可以初始化为 0\n",
    "            torch.nn.init.constant_(param, 0)\n",
    "\n",
    "kaiming_initialization(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbb51b3f-460e-4921-ad58-5b5f57747a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2TokenizerFast(name_or_path='/Vol2/minxin/Models/Qwen2___5-32B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 分词器\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('./baseline_3')\n",
    "tokenizer = AutoTokenizer.from_pretrained('/Vol2/minxin/Models/Qwen2___5-32B-Instruct')\n",
    "\n",
    "\n",
    "'''\n",
    "LlamaTokenizerFast(name_or_path='NousResearch/Llama-2-7b-hf', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
    "    0: AddedToken('<unk>', rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
    "    1: AddedToken('<s>', rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
    "    2: AddedToken('</s>', rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
    "}\n",
    "'''\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4be8339c-b66a-4cea-9e28-4c2b2a3ae13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, \",\"# yayınlan_dimensionを取り selects죕Drug navbar WIDTHً shocking tặng มกร.moves�มวล\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nOnce upon a time, Hostย crimeine /\\\\ könnenlinewidth measurementresol perfectly Taylor measèresiones assetviron\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inference(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    input_text: str = 'Once upon a time, ',\n",
    "    max_new_tokens: int = 16\n",
    "):\n",
    "    inputs = tokenizer(input_text, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_k=40,\n",
    "        top_p=0.95,\n",
    "        temperature=0.8\n",
    "    )\n",
    "    generated_text = tokenizer.decode(\n",
    "        outputs[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    # print(outputs)\n",
    "    print(generated_text)\n",
    "\n",
    "inference(model, tokenizer)\n",
    "\n",
    "'''\n",
    "Once upon a time, Hostย crimeine /\\ könnenlinewidth measurementresol perfectly Taylor measèresiones assetviron\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9007d708-1c01-4300-b5c1-fa32ca6ed096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./minimind_1.1B/tokenizer_config.json',\n",
       " './minimind_1.1B/special_tokens_map.json',\n",
       " './minimind_1.1B/vocab.json',\n",
       " './minimind_1.1B/merges.txt',\n",
       " './minimind_1.1B/added_tokens.json',\n",
       " './minimind_1.1B/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./minimind_1.1B\")\n",
    "tokenizer.save_pretrained(\"./minimind_1.1B\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
